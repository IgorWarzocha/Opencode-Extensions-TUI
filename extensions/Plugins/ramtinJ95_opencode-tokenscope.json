{
  "id": "ramtinJ95_opencode-tokenscope",
  "name": "TokenScope",
  "description": "Comprehensive token usage analysis and cost tracking for opencode sessions",
  "readme": "# OpenCode-Tokenscope, Token Analyzer Plugin\n\n> Comprehensive token usage analysis and cost tracking for OpenCode AI sessions\n\nTrack and optimize your token usage across system prompts, user messages, tool outputs, and more. Get detailed breakdowns, accurate cost estimates, and visual insights for your AI development workflow.\n\n## Features\n\n### Comprehensive Token Analysis\n- **5 Category Breakdown**: System prompts, user messages, assistant responses, tool outputs, and reasoning traces\n- **Visual Charts**: Easy-to-read ASCII bar charts with percentages and token counts\n- **Smart Inference**: Automatically infers system prompts from API telemetry (since they're not exposed in session messages)\n\n### Accurate Cost Tracking\n- **41+ Models Supported**: Comprehensive pricing database for Claude, GPT, DeepSeek, Llama, Mistral, and more\n- **Cache-Aware Pricing**: Properly handles cache read/write tokens with discounted rates\n- **Session-Wide Billing**: Aggregates costs across all API calls in your session\n\n### Subagent Cost Tracking\n- **Child Session Analysis**: Recursively analyzes all subagent sessions spawned by the Task tool\n- **Aggregated Totals**: Shows combined tokens, costs, and API calls across main session and all subagents\n- **Per-Agent Breakdown**: Lists each subagent with its type, token usage, cost, and API call count\n- **Optional Toggle**: Enable/disable subagent analysis with the `includeSubagents` parameter\n\n### Advanced Features\n- **Tool Usage Stats**: Track which tools consume the most tokens and how many times each is called\n- **API Call Tracking**: See total API calls for main session and subagents\n- **Top Contributors**: Identify the biggest token consumers\n- **Model Normalization**: Handles `provider/model` format automatically\n- **Multi-Tokenizer Support**: Uses official tokenizers (tiktoken for OpenAI, transformers for others)\n\n## Quick Install\n\n### One-Line Install (Recommended)\n\n```bash\ncurl -sSL https://raw.githubusercontent.com/ramtinJ95/opencode-tokenscope/main/plugin/install.sh | bash\n```\n\nThen restart OpenCode and run `/tokenscope`\n\n## Manual Installation\n\n<details>\n<summary>Click to expand manual installation steps</summary>\n\n### Requirements\n- OpenCode installed (`~/.config/opencode` directory exists)\n- npm (for tokenizer dependencies)\n- ~50MB disk space (for tokenizer models)\n\n### Installation Steps\n\n1. **Navigate to OpenCode config**:\n   ```bash\n   cd ~/.config/opencode\n   ```\n\n2. **Download plugin files**:\n   ```bash\n   # Download to plugin directory\n   cd plugin\n   curl -O https://raw.githubusercontent.com/ramtinJ95/opencode-tokenscope/main/plugin/tokenscope.ts\n   curl -O https://raw.githubusercontent.com/ramtinJ95/opencode-tokenscope/main/plugin/models.json\n   curl -O https://raw.githubusercontent.com/ramtinJ95/opencode-tokenscope/main/plugin/install.sh\n   curl -O https://raw.githubusercontent.com/ramtinJ95/opencode-tokenscope/main/plugin/package.json\n   ```\n\n3. **Download command file**:\n   ```bash\n   cd ../command\n   curl -O https://raw.githubusercontent.com/ramtinJ95/opencode-tokenscope/main/command/tokenscope.md\n   ```\n\n4. **Install dependencies**:\n   ```bash\n   cd ../plugin\n   chmod +x install.sh\n   ./install.sh\n   ```\n\n5. **Restart OpenCode**\n\n6. **Test**: Run `/tokenscope` in any session\n\n</details>\n\n## Updating\n\n### Quick Update (v1.2.1+)\n\nIf you have v1.2.1 or later installed, use the local update script:\n\n```bash\n~/.config/opencode/plugin/install.sh --update\n```\n\n### Update from v1.2.0 or Earlier\n\nUse the remote script (this will also install the local update script for future use):\n\n```bash\ncurl -sSL https://raw.githubusercontent.com/ramtinJ95/opencode-tokenscope/main/plugin/install.sh | bash -s -- --update\n```\n\nBoth methods download the latest plugin files while skipping dependency installation (faster).\n\n### Full Reinstall\n\nFor a full reinstall (if you're having issues):\n\n```bash\ncurl -sSL https://raw.githubusercontent.com/ramtinJ95/opencode-tokenscope/main/plugin/install.sh | bash\n```\n\n## Usage\n\n### Basic Command\n\nSimply type in OpenCode:\n```\n/tokenscope\n```\n\nThe plugin will:\n1. Analyze the current session\n2. Count tokens across all categories\n3. Analyze all subagent (Task tool) child sessions recursively\n4. Calculate costs based on API telemetry\n5. Display results in terminal\n6. Save detailed report to `token-usage-output.txt`\n\n### Options\n\nThe tool accepts optional parameters:\n\n- **sessionID**: Analyze a specific session instead of the current one\n- **limitMessages**: Limit entries shown per category (1-10, default: 3)\n- **includeSubagents**: Include subagent child session costs (default: true)\n\n### Reading the Full Report\n\n```bash\ncat token-usage-output.txt\n```\n\n### Example Output\n\n```\n═══════════════════════════════════════════════════════════════════════════\nToken Analysis: Session ses_50c712089ffeshuuuJPmOoXCPX\nModel: claude-opus-4-5\n═══════════════════════════════════════════════════════════════════════════\n\nTOKEN BREAKDOWN BY CATEGORY\n─────────────────────────────────────────────────────────────────────────\nEstimated using tokenizer analysis of message content:\n\nInput Categories:\n  SYSTEM    ██████████████░░░░░░░░░░░░░░░░    45.8% (22,367)\n  USER      ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░        0.8% (375)\n  TOOLS     ████████████████░░░░░░░░░░░░░░    53.5% (26,146)\n\n  Subtotal: 48,888 estimated input tokens\n\nOutput Categories:\n  ASSISTANT ██████████████████████████████     100.0% (1,806)\n\n  Subtotal: 1,806 estimated output tokens\n\nLocal Total: 50,694 tokens (estimated)\n\nTOOL USAGE BREAKDOWN\n─────────────────────────────────────────────────────────────────────────\nbash                 ██████████░░░░░░░░░░░░░░░░░░░░     34.0% (8,886)    4x\nread                 ██████████░░░░░░░░░░░░░░░░░░░░     33.1% (8,643)    3x\ntask                 ████████░░░░░░░░░░░░░░░░░░░░░░     27.7% (7,245)    4x\nwebfetch             █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░      4.9% (1,286)    1x\ntokenscope           ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░         0.3% (75)    2x\nbatch                ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░         0.0% (11)    1x\n\nTOP CONTRIBUTORS\n─────────────────────────────────────────────────────────────────────────\n• System (inferred from API)   22,367 tokens (44.1%)\n• bash                         8,886 tokens (17.5%)\n• read                         8,643 tokens (17.0%)\n• task                         7,245 tokens (14.3%)\n• webfetch                     1,286 tokens (2.5%)\n\n═══════════════════════════════════════════════════════════════════════════\nMOST RECENT API CALL\n─────────────────────────────────────────────────────────────────────────\n\nRaw telemetry from last API response:\n  Input (fresh):              2 tokens\n  Cache read:            48,886 tokens\n  Cache write:               54 tokens\n  Output:                   391 tokens\n  ───────────────────────────────────\n  Total:                 49,333 tokens\n\n═══════════════════════════════════════════════════════════════════════════\nSESSION TOTALS (All 15 API calls)\n─────────────────────────────────────────────────────────────────────────\n\nTotal tokens processed across the entire session (for cost calculation):\n\n  Input tokens:              10 (fresh tokens across all calls)\n  Cache read:           320,479 (cached tokens across all calls)\n  Cache write:           51,866 (tokens written to cache)\n  Output tokens:          3,331 (all model responses)\n  ───────────────────────────────────\n  Session Total:        375,686 tokens (for billing)\n\n═══════════════════════════════════════════════════════════════════════════\nESTIMATED SESSION COST (API Key Pricing)\n─────────────────────────────────────────────────────────────────────────\n\nYou appear to be on a subscription plan (API cost is $0).\nHere's what this session would cost with direct API access:\n\n  Input tokens:              10 × $5.00/M  = $0.0001\n  Output tokens:          3,331 × $25.00/M  = $0.0833\n  Cache read:           320,479 × $0.50/M  = $0.1602\n  Cache write:           51,866 × $6.25/M  = $0.3242\n─────────────────────────────────────────────────────────────────────────\nESTIMATED TOTAL: $0.5677\n\nNote: This estimate uses standard API pricing from models.json.\nActual API costs may vary based on provider and context size.\n\n═══════════════════════════════════════════════════════════════════════════\nSUBAGENT COSTS (4 child sessions, 23 API calls)\n─────────────────────────────────────────────────────────────────────────\n\n  docs                         $0.3190  (194,701 tokens, 8 calls)\n  general                      $0.2957  (104,794 tokens, 4 calls)\n  docs                         $0.2736  (69,411 tokens, 4 calls)\n  general                      $0.5006  (197,568 tokens, 7 calls)\n─────────────────────────────────────────────────────────────────────────\nSubagent Total:            $1.3888  (566,474 tokens, 23 calls)\n\n═══════════════════════════════════════════════════════════════════════════\nSUMMARY\n─────────────────────────────────────────────────────────────────────────\n\n                          Cost        Tokens          API Calls\n  Main session:      $    0.5677       375,686            15\n  Subagents:         $    1.3888       566,474            23\n─────────────────────────────────────────────────────────────────────────\n  TOTAL:             $    1.9565       942,160            38\n\n═══════════════════════════════════════════════════════════════════════════\n\n```\n## Supported Models\n\n**41+ models with accurate pricing:**\n\n### Claude Models\n- Claude Opus 4.5, 4.1, 4\n- Claude Sonnet 4, 4-5, 3.7, 3.5, 3\n- Claude Haiku 4-5, 3.5, 3\n\n### OpenAI Models\n- GPT-4, GPT-4 Turbo, GPT-4o, GPT-4o Mini\n- GPT-3.5 Turbo\n- GPT-5 and all its variations\n\n### Other Models\n- DeepSeek (R1, V2, V3)\n- Llama (3.1, 3.2, 3.3)\n- Mistral (Large, Small)\n- Qwen, Kimi, GLM, Grok\n- And more...\n\n**Free/Open models** are marked with zero pricing.\n\n## Customization\n\n### Add New Model Pricing\n\nEdit `~/.config/opencode/plugin/models.json`:\n\n```json\n{\n  \"your-model-name\": {\n    \"input\": 1.50,\n    \"output\": 5.00,\n    \"cacheWrite\": 0.50,\n    \"cacheRead\": 0.10\n  }\n}\n```\n\nSave the file and restart OpenCode. The plugin will automatically use the new pricing.\n\n### Update Existing Model Pricing\n\nSimply edit the values in `models.json` and restart OpenCode. No code changes needed!\n\n## How It Works\n\n### System Prompt Inference\nOpenCode doesn't expose system prompts in the session messages API. The plugin intelligently infers them using:\n\n```\nSystem Tokens = (API Input + Cache Read) - (User Tokens + Tool Tokens)\n```\n\nThis works because the API input includes everything sent to the model.\n\n### Dual Tracking\n- **Current Context**: Uses the most recent API call with non-zero tokens (matches TUI)\n- **Session Total**: Aggregates all API calls for accurate billing\n\n### Subagent Analysis\nThe plugin uses OpenCode's session API to:\n1. Fetch all child sessions spawned by the Task tool\n2. Recursively analyze nested subagents (subagents can spawn their own subagents)\n3. Aggregate tokens, costs, and API call counts\n4. Calculate estimated costs using the same pricing as the main session\n\n### Model Name Normalization\nAutomatically handles `provider/model` format (e.g., `qwen/qwen3-coder` → `qwen3-coder`)\n\n## Troubleshooting\n\n### \"Dependencies missing\" Error\n\nRun the installer:\n```bash\ncd ~/.config/opencode/plugin\n./install.sh\n```\n\n### Command Not Appearing\n\n1. Verify `tokenscope.md` exists:\n   ```bash\n   ls ~/.config/opencode/command/tokenscope.md\n   ```\n2. Restart OpenCode completely\n3. Check OpenCode logs for plugin errors\n\n### Wrong Token Counts\n\nThe plugin uses API telemetry (ground truth). If counts seem off:\n- **Expected ~2K difference from TUI**: Plugin analyzes before its own response is added\n- **Model detection**: Check that the model name is recognized in the output\n- **Tokenizer not installed**: Re-run `install.sh`\n\n### New Model Not Showing Correct Pricing\n\n1. Check if model exists in `models.json`\n2. Try exact match or prefix match (e.g., `claude-sonnet-4` matches `claude-sonnet-4-20250514`)\n3. Add entry to `models.json` if missing\n4. Restart OpenCode after editing `models.json`\n\n### Plugin Fails to Load\n\n1. Validate JSON syntax:\n   ```bash\n   cd ~/.config/opencode/plugin\n   node -e \"JSON.parse(require('fs').readFileSync('models.json', 'utf8'))\"\n   ```\n2. Check for trailing commas or syntax errors\n3. Plugin falls back to default pricing if file is invalid\n\n## Understanding the Numbers\n\n### Current Context vs Session Total\n\n- **Current Context**: What's in your context window right now\n  - Based on most recent API call\n  - Used to understand current memory usage\n\n- **Session Total**: All tokens processed in this session\n  - Sum of all API calls in the main session\n  - What you're billed for (main session only)\n  - Used for cost calculation\n\n### Subagent Totals\n\nWhen using the Task tool, OpenCode spawns subagent sessions. These are tracked separately:\n\n- **Subagent Tokens**: Combined tokens from all child sessions\n- **Subagent API Calls**: Total API calls made by all subagents\n- **Grand Total**: Main session + all subagents combined\n\nThe summary section shows a breakdown:\n```\n                          Cost        Tokens          API Calls\n  Main session:      $    0.5677       375,686            15\n  Subagents:         $    1.3888       566,474            23\n─────────────────────────────────────────────────────────────────────────\n  TOTAL:             $    1.9565       942,160            38\n```\n\n### Cache Tokens\n\n- **Cache Read**: Tokens retrieved from cache (discounted rate ~90% off)\n- **Cache Write**: Tokens written to cache (slight premium ~25% more)\n- **Note**: Cache write is a billing charge, not additional context tokens\n\n## Architecture\n\n### Core Components\n\n1. **TokenizerManager**: Loads and caches tokenizers (tiktoken, transformers)\n2. **ModelResolver**: Detects model and selects appropriate tokenizer\n3. **ContentCollector**: Extracts content from session messages, including tool call counts\n4. **TokenAnalysisEngine**: Counts tokens and applies API telemetry adjustments\n5. **CostCalculator**: Calculates costs from pricing database with cache-aware pricing\n6. **SubagentAnalyzer**: Recursively fetches and analyzes child sessions from Task tool calls\n7. **OutputFormatter**: Generates visual reports with charts and summaries\n\n## Privacy & Security\n\n- **All processing is local**: No session data sent to external services\n- **Tokenizers from official sources**:\n  - OpenAI tokenizers: npm registry\n  - Transformers: Hugging Face Hub\n- **Open source**: Audit the code yourself\n\n## Performance\n\n- **Fast**: Tokenizers cached after first load\n- **Parallel**: Categories processed concurrently\n- **Efficient**: Only analyzes on demand\n- **First-run download**: Transformers models download on demand (5-50MB per model)\n- **Subsequent runs**: Instant (uses cache)\n\n## Contributing\n\nContributions welcome! Ideas for enhancement:\n\n- Historical trend analysis\n- Export to CSV/JSON/PDF\n- Optimization suggestions\n- Custom categorization rules\n- Real-time monitoring with alerts\n- Compare sessions\n- Token burn rate calculation\n\n## Support\n\n- **Issues**: [GitHub Issues](https://github.com/ramtinJ95/opencode-tokenscope/issues)\n- **Discussions**: [GitHub Discussions](https://github.com/ramtinJ95/opencode-tokenscope/discussions)\n",
  "author": "ramtinJ95",
  "author_url": "https://github.com/ramtinJ95",
  "repository_url": "https://github.com/ramtinJ95/opencode-tokenscope",
  "category": "Plugins",
  "install_command": "curl -sSL https://raw.githubusercontent.com/ramtinJ95/opencode-tokenscope/main/install.sh | bash",
  "install_method": "bash",
  "featured": true
}